arguments: /Users/cindybishop/Documents/Face_Recognition/lib/src/align/align_dataset_mtcnn.py ../data/lfw ../data/lfw_mtcnn --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25
--------------------
git hash: b'b7016981442d632708186e535fd5523601881022'
--------------------
b'diff --git a/lib/src/create_face_embeddings.py b/lib/src/create_face_embeddings.py\nindex d6f9f24..a57a999 100644\n--- a/lib/src/create_face_embeddings.py\n+++ b/lib/src/create_face_embeddings.py\n@@ -86,7 +86,7 @@ def parse_arguments(argv):\n \n     parser.add_argument(\'--lfw_batch_size\', type=int,\n         help=\'Number of images to process in a batch in the LFW test set.\', default=100)\n-    parser.add_argument(\'--model\', type=str,default=\'/home/gpuuser/vinayak/models/facenet/src/ckpt/20170512-110547\', \n+    parser.add_argument(\'--model\', type=str,default=\'/Users/cindybishop/Documents/Face_recognition/src/ckpt/20170512-110547\',\n         help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n     parser.add_argument(\'--image_size\', type=int,\n         help=\'Image size (height, width) in pixels.\', default=160)\ndiff --git a/lib/src/retrieve.py b/lib/src/retrieve.py\nindex 5d5d587..d4f4b3d 100644\n--- a/lib/src/retrieve.py\n+++ b/lib/src/retrieve.py\n@@ -70,72 +70,73 @@ args = parser.parse_args()\n \n def align_face(img,pnet, rnet, onet):\n \n-\t\t        minsize = 20 # minimum size of face\n-\t\t        threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n-\t\t        factor = 0.709 # scale factor\n-\n-                        print("before img.size == 0")\t\n-                        if img.size == 0:\n-                                print("empty array")\n-\t\t\t\treturn False,img,[0,0,0,0]\n-\n-                        if img.ndim<2:\n-                            print(\'Unable to align\')\n-\n-                        if img.ndim == 2:\n-                            img = to_rgb(img)\n-\n-                        img = img[:,:,0:3]\n-\t\n-\t\t\tbounding_boxes, _ = detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n-\n-                        nrof_faces = bounding_boxes.shape[0]\n-\n-                        \n-                        if nrof_faces==0:\n-                            return False,img,[0,0,0,0]\n-                        else:\n-                            det = bounding_boxes[:,0:4]\n-                            det_arr = []\n-                            img_size = np.asarray(img.shape)[0:2]\n-                            if nrof_faces>1:\n-                                if args.detect_multiple_faces:\n-                                    for i in range(nrof_faces):\n-                                        det_arr.append(np.squeeze(det[i]))\n-                                else:\n-                                    bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\n-                                    img_center = img_size / 2\n-                                    offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\n-                                    offset_dist_squared = np.sum(np.power(offsets,2.0),0)\n-                                    index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\n-                                    det_arr.append(det[index,:])\n-                            else:\n-                                det_arr.append(np.squeeze(det))\n-                            if len(det_arr)>0:\n-                                    faces = []\n-                                    bboxes = []\n-\t\t                    for i, det in enumerate(det_arr):\n-\t\t                        det = np.squeeze(det)\n-\t\t                        bb = np.zeros(4, dtype=np.int32)\n-\t\t                        bb[0] = np.maximum(det[0]-args.margin/2, 0)\n-\t\t                        bb[1] = np.maximum(det[1]-args.margin/2, 0)\n-\t\t                        bb[2] = np.minimum(det[2]+args.margin/2, img_size[1])\n-\t\t                        bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\n-\t\t                        cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n-\t\t                        scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\'bilinear\')\n-\t\t                        misc.imsave("cropped.png", scaled)\n-                                        faces.append(scaled)\n-                                        bboxes.append(bb)\n-\t\t                        print("leaving align face")\n-\t\t                    return True,faces,bboxes\n+    minsize = 20 # minimum size of face\n+    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n+    factor = 0.709 # scale factor\n+\n+    print("before img.size == 0")\n+    if img.size == 0:\n+        print("empty array")\n+        return False,img,[0,0,0,0]\n+\n+    if img.ndim<2:\n+        print(\'Unable to align\')\n+\n+    if img.ndim == 2:\n+        img = to_rgb(img)\n+\n+    img = img[:,:,0:3]\n+\n+    bounding_boxes, _ = detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n+\n+    nrof_faces = bounding_boxes.shape[0]\n+\n+\n+    if nrof_faces==0:\n+        print("nrof_faces = 0")\n+        return False,img,[0,0,0,0]\n+    else:\n+        print("find bounding boxes")\n+        det = bounding_boxes[:,0:4]\n+        det_arr = []\n+        img_size = np.asarray(img.shape)[0:2]\n+        if nrof_faces>1:\n+            if args.detect_multiple_faces:\n+                for i in range(nrof_faces):\n+                    det_arr.append(np.squeeze(det[i]))\n+            else:\n+                bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\n+                img_center = img_size / 2\n+                offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\n+                offset_dist_squared = np.sum(np.power(offsets,2.0),0)\n+                index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\n+                det_arr.append(det[index,:])\n+        else:\n+            det_arr.append(np.squeeze(det))\n+        if len(det_arr)>0:\n+                faces = []\n+                bboxes = []\n+        for i, det in enumerate(det_arr):\n+            det = np.squeeze(det)\n+            bb = np.zeros(4, dtype=np.int32)\n+            bb[0] = np.maximum(det[0]-args.margin/2, 0)\n+            bb[1] = np.maximum(det[1]-args.margin/2, 0)\n+            bb[2] = np.minimum(det[2]+args.margin/2, img_size[1])\n+            bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\n+            cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n+            scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\'bilinear\')\n+            misc.imsave("/Users/cindybishop/Documents/Face_recognition/server/static/images/cropped.png", scaled)\n+            faces.append(scaled)\n+            bboxes.append(bb)\n+            print("leaving align face")\n+        return True,faces,bboxes\n \t\t\t\n \n def identify_person(image_vector, feature_array, k=9):\n-\t    top_k_ind = np.argsort([np.linalg.norm(image_vector-pred_row) \\\n-                            for ith_row, pred_row in enumerate(feature_array.values())])[:k]\n-            result = feature_array.keys()[top_k_ind[0]]\n-            acc = np.linalg.norm(image_vector-feature_array.values()[top_k_ind[0]])\n-            return result, acc\n+    top_k_ind = np.argsort([np.linalg.norm(image_vector-pred_row) for ith_row, pred_row in enumerate(feature_array.values())])[:k]\n+    result = list(feature_array.keys())[top_k_ind[0]]\n+    acc = np.linalg.norm(image_vector-list(feature_array.values())[top_k_ind[0]])\n+    return result, acc\n \n \n def recognize_face(sess,pnet, rnet, onet,feature_array):\n@@ -148,42 +149,50 @@ def recognize_face(sess,pnet, rnet, onet,feature_array):\n     image_size = args.image_size\n     embedding_size = embeddings.get_shape()[1]\n \n-    cap = cv2.VideoCapture(-1)\n-\n-    while(True):\n-        ret, frame = cap.read()\n-        gray = cv2.cvtColor(frame, 0)\n-        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n-            cap.release()\n-            cv2.destroyAllWindows()\n-            break\n-        if(gray.size > 0):\n-            print(gray.size)\n-            response, faces,bboxs = align_face(gray,pnet, rnet, onet)\n-            print(response)\n-            if (response == True):\n-                    for i, image in enumerate(faces):\n-                            bb = bboxs[i]\n-                            images = load_img(image, False, False, image_size)\n-                            feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n-                            feature_vector = sess.run(embeddings, feed_dict=feed_dict)\n-                            result, accuracy = identify_person(feature_vector, feature_array,8)\n-                            print(result.split("/")[2])\n-                            print(accuracy)\n-\n-                            if accuracy < 0.9:\n-                                cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n-                                W = int(bb[2]-bb[0])//2\n-                                H = int(bb[3]-bb[1])//2\n-                                cv2.putText(gray,"Hello "+result.split("/")[2],(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n-                            else:\n-                                cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n-                                W = int(bb[2]-bb[0])//2\n-                                H = int(bb[3]-bb[1])//2\n-                                cv2.putText(gray,"WHO ARE YOU ?",(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n-                            del feature_vector\n-\n-            cv2.imshow(\'img\',gray)\n-        else:\n-            continue\n+    cap = cv2.VideoCapture(0)\n+\n+    ret, frame = cap.read()\n+\n+    gray = cv2.cvtColor(frame, 0)\n+\n+    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n+        cap.release()\n+        cv2.destroyAllWindows()\n+        return ""\n+\n+    if(gray.size > 0):\n+        print(gray.size)\n+        response, faces,bboxs = align_face(gray,pnet, rnet, onet)\n+        print("align_face" + str(response))\n+\n+        #cv2.imshow(\'img\', gray) - in align we save the image so we don\'t need to see it here\n+        if (response == True):\n+\n+            for i, image in enumerate(faces):\n+                bb = bboxs[i]\n+\n+                print("now load image")\n+                if image is not None:\n+                    images = load_img(image, False, False, image_size)\n+                    feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n+                    feature_vector = sess.run(embeddings, feed_dict=feed_dict)\n+                    result, accuracy = identify_person(feature_vector, feature_array,8)\n+                    print(result)\n+                    print(result.split("/")[2])\n+                    print(accuracy)\n+\n+                    if accuracy < 0.9:\n+                        cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                        W = int(bb[2]-bb[0])//2\n+                        H = int(bb[3]-bb[1])//2\n+                        cv2.putText(gray,"Hello "+result.split("/")[2],(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                    else:\n+                        cv2.rectangle(gray,(bb[0],bb[1]),(bb[2],bb[3]),(255,255,255),2)\n+                        W = int(bb[2]-bb[0])//2\n+                        H = int(bb[3]-bb[1])//2\n+                        cv2.putText(gray,"WHO ARE YOU ?",(bb[0]+W-(W//2),bb[1]-7), cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1,cv2.LINE_AA)\n+                    del feature_vector\n+                    oldgray = gray\n+                    return result\n+    return ""\n \ndiff --git a/requirements.txt b/requirements.txt\nindex d956dab..0d310c2 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,5 +1,4 @@\n tensorflow\n-pickle\n numpy\n scipy\n scikit-learn\ndiff --git a/server/rest-server.py b/server/rest-server.py\nindex 5e86c41..8c23a38 100644\n--- a/server/rest-server.py\n+++ b/server/rest-server.py\n@@ -5,15 +5,17 @@\n # here as json and being branched out to each projects. Basic level of validation is also being done in this file. #                                                                                                                                  \t       \n #-------------------------------------------------------------------------------------------------------------------------------                                                                                                                              \n ################################################################################################################################\n-from flask import Flask, jsonify, abort, request, make_response, url_for,redirect, render_template\n-from flask.ext.httpauth import HTTPBasicAuth\n+from flask import Flask, jsonify, abort, request, make_response, url_for,redirect, render_template, send_from_directory\n+from flask_httpauth import HTTPBasicAuth\n from werkzeug.utils import secure_filename\n import os\n import sys\n import random\n+import cv2\n+sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))\n+\n from tensorflow.python.platform import gfile\n from six import iteritems\n-sys.path.append(\'..\')\n import numpy as np\n from lib.src import retrieve\n from lib.src.align import detect_face\n@@ -24,21 +26,28 @@ app = Flask(__name__, static_url_path = "")\n \n auth = HTTPBasicAuth()\n \n+global closest_match_filename\n+global closest_match_rock\n+\n+closest_match_filename = "nothing"\n+closest_match_rock = "nothingtoo"\n+\n #==============================================================================================================================\n #                                                                                                                              \n #    Loading the stored face embedding vectors for image retrieval                                                                 \n #                                                                          \t\t\t\t\t\t        \n #                                                                                                                              \n #==============================================================================================================================\n-with open(\'../lib/src/face_embeddings.pickle\',\'rb\') as f:\n-\t\t\t\t\t    \tfeature_array = pickle.load(f) \n+with open(\'/Users/cindybishop/Documents/Face_recognition/lib/src/extracted_dict.pickle\',\'rb\') as f:\n+\tfeature_array = pickle.load(f)\n \n-model_exp = \'../lib/src/ckpt/20170512-110547\'\n+model_exp = \'/Users/cindybishop/Documents/Face_recognition/lib/src/ckpt/\'\n graph_fr = tf.Graph()\n sess_fr = tf.Session(graph=graph_fr)\n+\n with graph_fr.as_default():\n-\tsaverf = tf.train.import_meta_graph(os.path.join(model_exp, \'model-20170512-110547.meta\'))\n-\tsaverf.restore(sess_fr, os.path.join(model_exp, \'model-20170512-110547.ckpt-250000\'))\n+\tsaverf = tf.train.import_meta_graph(os.path.join(model_exp, \'model-20180408-102900.meta\'))\n+\tsaverf.restore(sess_fr, os.path.join(model_exp, \'model-20180408-102900.ckpt-90\'))\n \tpnet, rnet, onet = detect_face.create_mtcnn(sess_fr, None)\n #==============================================================================================================================\n #                                                                                                                              \n@@ -46,10 +55,48 @@ with graph_fr.as_default():\n #                                                                                                 \n #                                                                                                                              \n #==============================================================================================================================\n+\n+def set_data():\n+    return render_template(\'main.html\', closest_match_filename=closest_match_filename)\n+\n+@app.route(\'/get_cropped\')\n+def get_cropped():\n+\tprint("in get cropped")\n+\treturn "/images/cropped.png"\n+\n+@app.route(\'/return_img\')\n+def return_img():\n+\tglobal closest_match_filename\n+\treturn \'<img src=\' + url_for(\'static\', filename=closest_match_filename) + \'>\'\n+\n+@app.route(\'/get_match\')\n+def get_match():\n+\tprint("in get_match")\n+\tfull_path = "/images/humans_mtcnn/" + closest_match_filename\n+\treturn full_path\n+\n+@app.route(\'/rock_it\')\n+def rock_it():\n+\tprint("in rock it")\n+\tglobal closest_match_rock\n+\trock_number = closest_match_filename.rsplit("_", 1)[1]\n+\trock_number = rock_number.split(".")[0] + ".jpg" # take off png and add jpg\n+\tclosest_match_rock = "/images/rocks/rocks_" + rock_number\n+\tprint(closest_match_rock)\n+\treturn closest_match_rock\n+\n @app.route(\'/facerecognitionLive\', methods=[\'GET\', \'POST\'])\n def face_det():\n-    \n-    retrieve.recognize_face(sess_fr,pnet, rnet, onet,feature_array)\n+\n+\tclosest_match = retrieve.recognize_face(sess_fr,pnet, rnet, onet,feature_array)\n+\tprint("now save match")\n+\tclosest_match = "/".join(closest_match.split(\'/\')[8:])\n+\tglobal closest_match_filename\n+\tclosest_match_filename= closest_match\n+\tprint(closest_match_filename)\n+\trock_it()\n+\t# set_data() # I don\'t think this does anything. filename is still default "nothing"\n+\treturn closest_match_filename\n \n #==============================================================================================================================\n #                                                                                                                              \n@@ -58,7 +105,7 @@ def face_det():\n #==============================================================================================================================\n @app.route("/")\n def main():\n-    \n-    return render_template("main.html")   \n+\n+\treturn render_template("main.html", closest_match_filename=closest_match_filename, closest_match_rock=closest_match_rock)\n if __name__ == \'__main__\':\n-    app.run(debug = True, host= \'0.0.0.0\')\n+\tapp.run(debug = True, host= \'0.0.0.0\')\ndiff --git a/server/templates/main.html b/server/templates/main.html\nindex 7b072fc..aae7479 100644\n--- a/server/templates/main.html\n+++ b/server/templates/main.html\n@@ -3,20 +3,20 @@\n <html>\n <head>\n <title>Face Recognition Service</title>\n-<meta name="viewport" content="width=device-width, initial-scale=1.0">\n+<meta name="viewport" content="width=device-width, user-scalable=yes, initial-scale=1, maximum-scale=1">\n <link href="http://netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" rel="stylesheet">\n \n <script src="http://ajax.aspnetcdn.com/ajax/jquery/jquery-1.9.0.js"></script>\n <script src="http://netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>\n <script src="http://ajax.aspnetcdn.com/ajax/knockout/knockout-2.2.1.js"></script>\n+<script src="https://webrtc.github.io/adapter/adapter-latest.js"></script>\n+<script src="/js/main.js" async></script>\n \n+    <base target="_blank">\n </head>\n \n-<body style="background: white;>\n-\n-    <div class="navbar">\n-        <div class="navbar-inner">\n-            <h1 class="text-center" style="color:black;">Deep Learning Application Demos&nbsp;&nbsp;&nbsp;\n+<body>\n+        <div class="navbar-inner"><h1>Rock and Roll Spirit: how much do you rock?\n \t    </h1>\n \t    \n         </div>\n@@ -24,25 +24,56 @@\n     <div><br></br><br></br>\n     </div>\n \n-<div id="main" class="container"><center> <table><tr><td>\n- <center> \n-            \n-          \n-     <br><table style="background: white;border: 1px solid beige;box-shadow: 3px 5px 15px 0px rgba(0, 0, 0, 0.2), 3px 5px 15px 0 rgba(0, 0, 0, 0.19);\n-"><tr style="background: lightgrey;"><td><center>Face Recognition</center></td></tr><tr><td><input type="image" src="/images/face_recog.png" name="saveForm"  style="height:200px;width:200px;" onclick="call_server()" /></a> </td></tr></table> \n-     </center> \n+<div id="main" class="container">\n+\n+    <video playsinline autoplay></video>\n+\n+\n+     <br>\n+    <p>Face Recognition</p>\n+    <button id="showVideo">Show</button>\n+    <button id="snapshot">Take Pic</button>\n+    <input type="image" src="/images/face_recog.png" name="saveForm"  style="height:200px;width:200px;" onclick="call_server()" /></a>\n+\n+    <div>{{ url_for(\'return_img\') }}</div>\n+    <canvas></canvas>\n \n+    <img id="cropped" src="/images/cropped.png"></img>\n+     <img id="match" src="/images/humans_mtcnn/{{ closest_match_filename}}"></img>\n+    <img id="rock" src="{{ closest_match_rock}}"></img>\n     </div>\n-  \n-   \n-  \n-   \n+\n     <script type="text/javascript">\n \n-function call_server(){    \n-       \t\t\t$.ajax({\n-      \t\t url: \'facerecognitionLive\',\n-})};\n+        function load_new_rock() {\n+            $.ajax({\n+                url: \'rock_it\',\n+            }).done(function (data) {\n+                console.log(data)\n+                $("rock").src = data;\n+            });\n+        }\n+\n+        function load_new_matches() {\n+            $.ajax({\n+                url: \'get_match\',\n+            }).done(function (data) {\n+                console.log(data)\n+                //load_new_rock();\n+                $("match").src = data;\n+            });\n+        }\n+\n+\n+function call_server(){\n+    $.ajax({\n+        url: \'facerecognitionLive\',\n+    }).done(function() {\n+        //load_new_matches();\n+        //load_new_rock();\n+    });\n+}\n+'